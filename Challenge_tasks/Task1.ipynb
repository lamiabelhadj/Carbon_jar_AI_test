{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a2524d",
   "metadata": {},
   "source": [
    "1. Designing AI-Powered Emissions Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e58eb65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the LSTM Autoencoder\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=32, n_layers=1):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Encode\n",
    "        _, (hidden, cell) = self.encoder(x)\n",
    "\n",
    "        # Repeat the hidden state for each timestep in the sequence\n",
    "        decoder_input = hidden.repeat(seq_len, 1, 1).permute(1, 0, 2)\n",
    "\n",
    "        # Decode\n",
    "        decoded_seq, _ = self.decoder(decoder_input, (hidden, cell))\n",
    "\n",
    "        # Project back to original input dimension\n",
    "        out = self.output_layer(decoded_seq)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2021dfe4",
   "metadata": {},
   "source": [
    "The encoder reads in the full sequence and compresses it into a final hidden state.\n",
    "\n",
    "We then use that hidden state to initialize the decoder, which reconstructs the sequence.\n",
    "\n",
    "Since the decoder needs one input per timestep, we repeat the hidden state across the sequence length. This is a simple way to decode without using teacher forcing or autoregressive decoding, which can be added later for better results.\n",
    "\n",
    "The output layer maps the hidden dimension back to the input size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d9a385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5110\n",
      "Epoch 2/10, Loss: 0.5095\n",
      "Epoch 3/10, Loss: 0.5081\n",
      "Epoch 4/10, Loss: 0.5069\n",
      "Epoch 5/10, Loss: 0.5057\n",
      "Epoch 6/10, Loss: 0.5046\n",
      "Epoch 7/10, Loss: 0.5036\n",
      "Epoch 8/10, Loss: 0.5027\n",
      "Epoch 9/10, Loss: 0.5019\n",
      "Epoch 10/10, Loss: 0.5011\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "input_dim = 5\n",
    "seq_len = 20\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Generate simulated time-series data\n",
    "np.random.seed(42)\n",
    "sim_data = np.sin(np.linspace(0, 100, seq_len)) + np.random.normal(0, 0.1, seq_len)\n",
    "sim_data = np.tile(sim_data.reshape(1, seq_len, 1), (batch_size, 1, input_dim))  # shape: (batch, seq_len, input_dim)\n",
    "train_data = torch.tensor(sim_data, dtype=torch.float32)\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "model = LSTMAutoencoder(input_dim=input_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(train_data)\n",
    "    loss = criterion(output, train_data)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f23efe6",
   "metadata": {},
   "source": [
    "Training Progress:\n",
    "The model's reconstruction loss (MSE) is decreasing gradually over epochs.\n",
    "This indicates that the autoencoder is learning to better reconstruct the input time series.\n",
    "A slow but consistent drop in loss suggests stable learning and no signs of overfitting or divergence.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
